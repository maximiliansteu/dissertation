{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Feature selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# NLTK Imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment import util\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Others\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Readability\n",
    "import textstat\n",
    "# Boxplot\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Classififer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Official Communication vs. Fake News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features \n",
    "features_real_and_fake = feature_df_real_and_fake.columns.drop('label').drop('id').drop('length_preprocessed').drop('preprocessed')\n",
    "print(features_real_and_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of rows and columns for the subplot grid\n",
    "n = len(features_real_and_fake)\n",
    "ncols = 2\n",
    "nrows = n // ncols + (n % ncols > 0)\n",
    "\n",
    "# Create the subplots\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(15, nrows*5))\n",
    "axs = axs.flatten()  # Flatten to make indexing easier\n",
    "\n",
    "for i, feature in enumerate(features_real_and_fake):\n",
    "    # Calculate median and standard deviation for the feature by class\n",
    "    median_fake = feature_df_real_and_fake[feature_df_real_and_fake['label'] == 1][feature].median()\n",
    "    std_dev_fake = feature_df_real_and_fake[feature_df_real_and_fake['label'] == 1][feature].std()\n",
    "\n",
    "    median_real = feature_df_real_and_fake[feature_df_real_and_fake['label'] == -1][feature].median()\n",
    "    std_dev_real = feature_df_real_and_fake[feature_df_real_and_fake['label'] == -1][feature].std()\n",
    "\n",
    "    # Create the boxplot\n",
    "    sns.boxplot(x='label', y=feature, data=feature_df_real_and_fake, ax=axs[i])\n",
    "\n",
    "    # Set the title with median and standard deviation\n",
    "    axs[i].set_title(f'{feature}')\n",
    "\n",
    "    # Add the calculated stats in the plot\n",
    "    axs[i].text(0.5, 0.9, f'Fake News - Median: {median_fake:.2f}, Std Dev: {std_dev_fake:.2f}',\n",
    "                transform=axs[i].transAxes)\n",
    "    axs[i].text(0.5, 0.8, f'Real News - Median: {median_real:.2f}, Std Dev: {std_dev_real:.2f}',\n",
    "                transform=axs[i].transAxes)\n",
    "\n",
    "# Remove extra subplots\n",
    "if len(features_real_and_fake) < len(axs):\n",
    "    for i in range(len(features), len(axs)):\n",
    "        fig.delaxes(axs[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the feature_df\n",
    "feature_df = feature_df.sample(frac=1, random_state=42)\n",
    "feature_df_real_and_fake = feature_df_real_and_fake.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear SVM\n",
    "\n",
    "# Define your feature set and target variable\n",
    "features = ['sentiment_vader', 'readability', 'lexical_diversity', 'superlatives', 'exclamation_marks']\n",
    "X = feature_df[features]\n",
    "y = feature_df['label']\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (shuffle by default)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the model\n",
    "model = LinearSVC(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Sequential backward selection\n",
    "sbs = SFS(model, \n",
    "           k_features=1, \n",
    "           forward=False, \n",
    "           floating=False, \n",
    "           scoring='f1_macro',\n",
    "           cv=5)\n",
    "\n",
    "sbs = sbs.fit(X_train, y_train)\n",
    "\n",
    "# Get the final set of features\n",
    "final_features = list(sbs.k_feature_names_)\n",
    "print('Final features:', final_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So what is going on? \n",
    "for k in sbs.subsets_:\n",
    "    print(f'Number of features: {k}')\n",
    "    print('Selected features:', sbs.subsets_[k]['feature_names'])\n",
    "    print('CV score:', sbs.subsets_[k]['avg_score'])\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model with final features and get accuracy\n",
    "model.fit(X_train[['sentiment_vader','readability','lexical_diversity']], y_train)\n",
    "y_pred = model.predict(X_test[['sentiment_vader', 'readability', 'lexical_diversity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f'The F1 score of the Linear SVM model with selected features is: {f1:.2f}')\n",
    "\n",
    "# Calculate and print precision\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "print(f'The precision of the Linear SVM model with selected features is: {precision:.2f}')\n",
    "\n",
    "# Calculate and print recall\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "print(f'The recall of the Linear SVM model with selected features is: {recall:.2f}')\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real vs. Fake News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your feature set and target variable\n",
    "features = ['sentiment_vader', 'readability', 'lexical_diversity', 'superlatives']\n",
    "X_real_and_fake = feature_df_real_and_fake[features]\n",
    "y_real_and_fake = feature_df_real_and_fake['label']\n",
    "\n",
    "print(X_real_and_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (shuffle by default)\n",
    "X_train_real_and_fake, X_test_real_and_fake, y_train_real_and_fake, y_test_real_and_fake = train_test_split(X_real_and_fake,\n",
    "                                                    y_real_and_fake,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the model\n",
    "model_real_and_fake = LinearSVC(random_state=42)\n",
    "\n",
    "# Sequential backward selection\n",
    "sbs_real_and_fake = SFS(model_real_and_fake, \n",
    "           k_features=1, \n",
    "           forward=False, \n",
    "           floating=False, \n",
    "           scoring='accuracy',\n",
    "           cv=5)\n",
    "\n",
    "sbs_real_and_fake = sbs_real_and_fake.fit(X_train_real_and_fake, y_train_real_and_fake)\n",
    "\n",
    "# Get the final set of features\n",
    "final_features = list(sbs.k_feature_names_)\n",
    "print('Final features:', final_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So what is going on? \n",
    "for k in sbs_real_and_fake.subsets_:\n",
    "    print(f'Number of features: {k}')\n",
    "    print('Selected features:', sbs_real_and_fake.subsets_[k]['feature_names'])\n",
    "    print('CV score:', sbs_real_and_fake.subsets_[k]['avg_score'])\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model with final features and get accuracy\n",
    "model_real_and_fake.fit(X_train_real_and_fake[['sentiment_vader','readability','lexical_diversity']], y_train_real_and_fake)\n",
    "y_pred_real_and_fake = model_real_and_fake.predict(X_test_real_and_fake[['sentiment_vader', 'readability', 'lexical_diversity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print Accuracy\n",
    "acc_real_and_fake = accuracy_score(y_test_real_and_fake, y_pred_real_and_fake)\n",
    "print(f'The Accuracy score of the Linear SVM model with selected features is: {acc_real_and_fake:.2f}')\n",
    "\n",
    "# Print confusion matrix\n",
    "cm_real_and_fake = confusion_matrix(y_test_real_and_fake, y_pred_real_and_fake)\n",
    "print('Confusion Matrix:')\n",
    "print(cm_real_and_fake)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Official Communication vs. Fake News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define previously chosen features \n",
    "features = ['sentiment_vader','readability','lexical_diversity']\n",
    "X = feature_df[features]\n",
    "y = feature_df['label']\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (shuffle by default)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.1, 10, 1000], \n",
    "              'gamma': [0.1, 0.001],\n",
    "              'kernel': ['rbf', 'poly']}\n",
    "\n",
    "# Create a SVC model\n",
    "svc = SVC(probability=True, class_weight='balanced')\n",
    "\n",
    "# Use Stratified CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create the GridSearchCV model\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=cv, verbose=3, n_jobs=-1, scoring='f1_macro')\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Get the best score\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best score: {best_score}\")\n",
    "\n",
    "# Use the best estimator for predictions\n",
    "best_svc = grid_search.best_estimator_\n",
    "y_pred = best_svc.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f'The F1 score of the Linear SVM model with selected features is: {f1:.2f}')\n",
    "\n",
    "# Calculate and print precision\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "print(f'The precision of the Linear SVM model with selected features is: {precision:.2f}')\n",
    "\n",
    "# Calculate and print recall\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "print(f'The recall of the Linear SVM model with selected features is: {recall:.2f}')\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
